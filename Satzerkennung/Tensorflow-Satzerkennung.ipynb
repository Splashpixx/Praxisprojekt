{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom IPython import display\nfrom jiwer import wer\nimport os as os\nfrom time import time\nimport tensorflow_io as tfio\n\nfrom IPython.display import FileLink\n# pip install jiwer | Always in Kaggle","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\ndata_path = \"/kaggle/input/librispeech/LJSpeech-1.1\"\nwavs_path = data_path + \"/wavs/\"\nmetadata_path = data_path + \"/metadata.csv\"\n\n# Read metadata file and parse it\nmetadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\nmetadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\nmetadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\nmetadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\nmetadata_df.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split date into train and validation\nsplit = int(len(metadata_df) * 0.90)\ndf_train = metadata_df[:split]\ndf_val = metadata_df[split:]\n\nprint(f\"Size of the training set: {len(df_train)}\")\nprint(f\"Size of the training set: {len(df_val)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Pre Processing**","metadata":{}},{"cell_type":"code","source":"characters = [x for x in \"abcdefghijklmnopqrstuvwxyz'?! \"]\n# Mapping characters to integers\nchar_to_num = keras.layers.StringLookup(vocabulary=characters, oov_token=\"\")\n# Mapping integers back to original characters\nnum_to_char = keras.layers.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The window length\nframe_length = 256\n#number of samples to step.\nframe_step = 160\n#size of the FFT\nfft_length = 384\n\ndef encode_single_sample(wav_file, label):\n    #  Audio Part\n    # 1. Read wav file\n    file = tf.io.read_file(wavs_path + wav_file + \".wav\")\n    # 2. Decode the wav file\n    audio, _ = tf.audio.decode_wav(file)\n    audio = tf.squeeze(audio, axis=-1)\n    # 3. Change type to float\n    audio = tf.cast(audio, tf.float32)\n    # 4. Get the spectrogram\n    spectrogram = tf.signal.stft(\n        audio, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length\n    )\n    # 5. magnitude\n    spectrogram = tf.abs(spectrogram)\n    spectrogram = tf.math.pow(spectrogram, 0.5)\n    # 6. normalisation\n    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n    \n    \n    # Text Part    \n    # 7. Convert label to Lower case\n    label = tf.strings.lower(label)\n    # 8. Split the label\n    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n    # 9. Map the characters in label to numbers\n    label = char_to_num(label)\n    # 10. Return a dict as our model is expecting two inputs\n    return spectrogram, label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating Dataset**","metadata":{}},{"cell_type":"code","source":"batch_size = 32\n# Define the trainig dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n    (list(df_train[\"file_name\"]), list(df_train[\"normalized_transcription\"]))\n)\n\ntrain_dataset = (\n    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    .padded_batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\nprint(train_dataset.map)\n# Define the validation dataset\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n    (list(df_val[\"file_name\"]), list(df_val[\"normalized_transcription\"]))\n)\nvalidation_dataset = (\n    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n    .padded_batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Visualize*","metadata":{}},{"cell_type":"code","source":"fig = plt.figure(figsize=(8, 5))\nfor batch in train_dataset.take(1):\n    spectrogram = batch[0][0].numpy()\n    spectrogram = np.array([np.trim_zeros(x) for x in np.transpose(spectrogram)])\n    label = batch[1][0]\n    # Spectrogram\n    label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n    ax = plt.subplot(2, 1, 1)\n    ax.imshow(spectrogram, vmax=1)\n    ax.set_title(label)\n    ax.axis(\"off\")\n    # Wav\n    file = tf.io.read_file(wavs_path + list(df_train[\"file_name\"])[0] + \".wav\")\n    audio, _ = tf.audio.decode_wav(file)\n    audio = audio.numpy()\n    ax = plt.subplot(2, 1, 2)\n    plt.plot(audio)\n    ax.set_title(\"Signal Wave\")\n    ax.set_xlim(0, len(audio))\n    display.display(display.Audio(np.transpose(audio), rate=16000))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"code","source":"def CTCLoss(y_true, y_pred):\n    # Compute the training-time loss value\n    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n    loss = keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(input_dim, output_dim, rnn_layers=5, rnn_units=128):\n\n    # Model's input\n    input_spectrogram = layers.Input((None, input_dim), name=\"input\")\n    # Expand the dimension to use 2D CNN.\n    x = layers.Reshape((-1, input_dim, 1), name=\"expand_dim\")(input_spectrogram)\n    # Convolution layer 1\n    x = layers.Conv2D(\n        filters=32,\n        kernel_size=[11, 41],\n        strides=[2, 2],\n        padding=\"same\",\n        use_bias=False,\n        name=\"conv_1\",\n    )(x)\n    x = layers.BatchNormalization(name=\"conv_1_bn\")(x)\n    x = layers.ReLU(name=\"conv_1_relu\")(x)\n    # Convolution layer 2\n    x = layers.Conv2D(\n        filters=32,\n        kernel_size=[11, 21],\n        strides=[1, 2],\n        padding=\"same\",\n        use_bias=False,\n        name=\"conv_2\",\n    )(x)\n    x = layers.BatchNormalization(name=\"conv_2_bn\")(x)\n    x = layers.ReLU(name=\"conv_2_relu\")(x)\n    # Reshape the resulted volume to feed the RNNs layers\n    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n    # RNN layers\n    for i in range(1, rnn_layers + 1):\n        recurrent = layers.GRU(\n            units=rnn_units,\n            activation=\"tanh\",\n            recurrent_activation=\"sigmoid\",\n            use_bias=True,\n            return_sequences=True,\n            reset_after=True,\n            name=f\"gru_{i}\",\n        )\n        x = layers.Bidirectional(\n            recurrent, name=f\"bidirectional_{i}\", merge_mode=\"concat\"\n        )(x)\n        if i < rnn_layers:\n            x = layers.Dropout(rate=0.5)(x)\n    # Dense layer\n    x = layers.Dense(units=rnn_units * 2, name=\"dense_1\")(x)\n    x = layers.ReLU(name=\"dense_1_relu\")(x)\n    x = layers.Dropout(rate=0.5)(x)\n    # Classification layer\n    output = layers.Dense(units=output_dim + 1, activation=\"softmax\")(x)\n    # Model\n    model = keras.Model(input_spectrogram, output, name=\"MyFirstASRModel\")\n    # Optimizer\n    opt = keras.optimizers.Adam(learning_rate=1e-4)\n    # Compile the model and return\n    model.compile(optimizer=opt, loss=CTCLoss)\n    return model\n\n\n# Get the model\nmodel = build_model(\n    input_dim=fft_length // 2 + 1,\n    output_dim=char_to_num.vocabulary_size(),\n    rnn_units=512,\n)\nmodel.summary(line_length=110)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the Model**","metadata":{}},{"cell_type":"code","source":"def decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search\n    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0]\n    # Iterate over the results and get back the text\n    output_text = []\n    for result in results:\n        result = tf.strings.reduce_join(num_to_char(result)).numpy().decode(\"utf-8\")\n        output_text.append(result)\n    return output_text\n\n\n# A callback class to output a few transcriptions during training\nclass CallbackEval(keras.callbacks.Callback):\n    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n\n    def __init__(self, dataset):\n        super().__init__()\n        self.dataset = dataset\n\n    def on_epoch_end(self, epoch: int, logs=None):\n        predictions = []\n        targets = []\n        for batch in self.dataset:\n            X, y = batch\n            batch_predictions = model.predict(X)\n            batch_predictions = decode_batch_predictions(batch_predictions)\n            predictions.extend(batch_predictions)\n            for label in y:\n                label = (\n                    tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n                )\n                targets.append(label)\n        wer_score = wer(targets, predictions)\n        print(\"-\" * 100)\n        print(f\"Word Error Rate: {wer_score:.4f}\")\n        print(\"-\" * 100)\n        for i in np.random.randint(0, len(predictions), 2):\n            print(f\"Target    : {targets[i]}\")\n            print(f\"Prediction: {predictions[i]}\")\n            print(\"-\" * 100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load\nmodel=tf.keras.models.load_model('/kaggle/input/modelepoch18/my_modelEpoch18.hdf5', custom_objects={'CTCLoss': CTCLoss})\n\n# Define the number of epochs.\nepochs = 3\n# Callback function to check transcription on the val set.\nvalidation_callback = CallbackEval(validation_dataset)\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    callbacks=[validation_callback],\n)\n# safe the model\ntf.keras.models.save_model(model,'teLiteMod.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classicModel = tf.keras.models.load_model(\"/kaggle/input/finalmodel/teLiteMod.h5\", custom_objects={'CTCLoss': CTCLoss})\nprint(\"done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def myinput(audiopath):\n    \n    frame_length = 256\n    frame_step = 160\n    fft_length = 384\n    \n    selectedAudio = audiopath # Audio File\n\n    x = tf.io.read_file(selectedAudio)\n    x, _ = tf.audio.decode_wav(x)\n    x = tf.squeeze(x, axis=-1)\n    x = tf.cast(x, tf.float32)\n\n    spectrogram = tf.signal.stft( x, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length )\n    spectrogram = tf.abs(spectrogram)\n    spectrogram = tf.math.pow(spectrogram, 0.5)\n    means = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\n    stddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\n    spectrogram = (spectrogram - means) / (stddevs + 1e-10)\n    \n    label = tf.strings.lower(\"label\")\n    \n    label = tf.strings.unicode_split(label, input_encoding=\"UTF-8\")\n    \n    label = char_to_num(label)\n    \n    return spectrogram, label\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The window length\nframe_length = 256\n#number of samples to step.\nframe_step = 160\n#size of the FFT\nfft_length = 384\nSAMPLE = 22050\nselectedAudio = \"/kaggle/input/librispeech/LJSpeech-1.1/wavs/LJ001-0006.wav\" # Audio File\n\naudio_file = tf.io.read_file(selectedAudio)\naudio, _ = tf.audio.decode_wav(audio_file, desired_channels=1)\naudio = tf.squeeze(audio, axis=-1)\naudio = tf.cast(sample, tf.float32)\n\n#audio = tfio.audio.resample( audio, rate_in=sample, rate_out=SAMPLE )\n#spectrogram = tfio.audio.spectrogram(waveform, nfft=512, window=512, stride=130)\n\nspectrogram = tf.signal.stft( audio_tensor, frame_length=frame_length, frame_step=frame_step, fft_length=fft_length )\n\nspectrogram = tf.abs(spectrogram)\nspectrogram = tf.math.pow(spectrogram, 0.5)\n\nmeans = tf.math.reduce_mean(spectrogram, 1, keepdims=True)\nstddevs = tf.math.reduce_std(spectrogram, 1, keepdims=True)\nspectrogram = (spectrogram - means) / (stddevs + 1e-10)\nspectrogram = tf.expand_dims(spectrogram, axis=0)\n\nprint(spectrogram)\ntime_before = time()\n\nkeras_predic = classicModel.predict(spectrogram)\nkeras_predic = decode_batch_predictions(keras_predic)\n\ntime_after = time()\n\ntotal_time = time_after - time_before\ndisplay.display(display.Audio(selectedAudio, rate=16000))\n\n\nprint(\"The Prediction is:\", predictions)\nprint(\"Total time:\", total_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}