{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nimport tensorflow_io as tfio\n\nfrom IPython import display\nfrom IPython.display import Audio\nfrom time import time\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATASET_PATH = '/kaggle/input/mini-speech2/mini_speech_commands'\ndata_dir = pathlib.Path(DATASET_PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n    directory=data_dir,\n    batch_size=128,\n    validation_split=0.2,\n    seed=0,\n    output_sequence_length=16000,\n    subset='both')\n\nlabel_names = np.array(train_ds.class_names)\n#print(\"label names:\", label_names)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def squeeze(audio, labels):\n    audio = tf.squeeze(audio, axis=-1)\n    return audio, labels\n\ntrain_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\nval_ds = val_ds.map(squeeze, tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = val_ds.shard(num_shards=2, index=0)\nval_ds = val_ds.shard(num_shards=2, index=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Convert waveforms to spectrograms**","metadata":{}},{"cell_type":"code","source":"def get_spectrogram(waveform):\n    # Convert the waveform to a spectrogram. SFTF -> returning a 2D Tensor.\n    spectrogram = tfio.audio.spectrogram(waveform, nfft=512, window=512, stride=130)\n    # Obtain the magnitude.\n    spectrogram = tf.abs(spectrogram)\n    # shape (`batch_size`, `height`, `width`, `channels`).\n    spectrogram = spectrogram[..., tf.newaxis]\n    \n    return spectrogram","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example_audio, example_labels in train_ds.take(1):  \n    break\n\nfor i in range(3):\n    label = label_names[example_labels[i]]\n    waveform = example_audio[i]\n    spectrogram = get_spectrogram(waveform)\n    \n    #print(waveform)\n    print('Label:', label)\n    print('Waveform shape:', waveform.shape)\n    print('Spectrogram shape:', spectrogram.shape)\n    print('Audio playback')\n    display.display(display.Audio(waveform, rate=16000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Mel-Spectogram**","metadata":{}},{"cell_type":"code","source":"def timeMasking(audio):\n    time_mask = tfio.audio.time_mask(audio, param=8)\n    return time_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def specAugment(audio):\n    freq_mask = tfio.audio.freq_mask(audio, param=8)\n    return freq_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_mel_specto(audio):\n        \n    spectrogram = tfio.audio.spectrogram(audio, nfft=512, window=300, stride=130)\n    #spectrogram = tfio.audio.spectrogram(audio, nfft=512, window=512, stride=130)\n    \n    # Convert to mel-spectrogram\n    mel_spectrogram = tfio.audio.melscale(\n        spectrogram, rate=32000, mels=128, fmin=0, fmax=14000) \n    # spectrogram, rate=16000, mels=129, fmin=0, fmax=7000) \n\n    # Convert to db scale mel-spectrogram\n    dbscale_mel_spectrogram = tfio.audio.dbscale(\n        mel_spectrogram, top_db=85)\n    \n    dbscale_mel_spectrogram = tf.abs(dbscale_mel_spectrogram) # * -1 wenn zahl < 0\n    \n    dbscale_mel_spectrogram = dbscale_mel_spectrogram[..., tf.newaxis]\n    return dbscale_mel_spectrogram # Output -> ( stride , mels, tf.newaxis )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Display Spectogram**","metadata":{}},{"cell_type":"code","source":"def plot_spectrogram(spectrogram, ax):\n    if len(spectrogram.shape) > 2:\n        assert len(spectrogram.shape) == 3\n        spectrogram = np.squeeze(spectrogram, axis=-1)\n    \n    log_spec = np.log(spectrogram.T + np.finfo(float).eps)\n    height = log_spec.shape[0]\n    width = log_spec.shape[1]\n    X = np.linspace(0, np.size(spectrogram), num=width, dtype=int)\n    Y = range(height)\n    ax.pcolormesh(X, Y, log_spec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normal spectrogram => get_spectrogram(audio)\n# Mel-Spec => create_mel_specto(audio)\ndef make_spec_ds(ds):\n    return ds.map(\n        map_func=lambda audio,label: (get_spectrogram(audio), label),\n        num_parallel_calls=tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_spectrogram_ds = make_spec_ds(train_ds)\nval_spectrogram_ds = make_spec_ds(val_ds)\ntest_spectrogram_ds = make_spec_ds(test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example_spectrograms, example_spect_labels in train_spectrogram_ds.take(1):\n    break\n    \nrows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(16, 9))\n\nfor i in range(n):\n    r = i // cols\n    c = i % cols\n    ax = axes[r][c]\n    plot_spectrogram(example_spectrograms[i], ax)\n    ax.set_title(label_names[example_spect_labels[i]])\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Build and Train**","metadata":{}},{"cell_type":"code","source":"train_spectrogram_ds = train_spectrogram_ds.cache().shuffle(10000).prefetch(tf.data.AUTOTUNE)\nval_spectrogram_ds = val_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)\ntest_spectrogram_ds = test_spectrogram_ds.cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = example_spectrograms.shape[1:]\nprint('Input shape:', input_shape)\nnum_labels = len(label_names)\n\n# Instantiate the `tf.keras.layers.Normalization` layer.\nnorm_layer = layers.Normalization()\nnorm_layer.adapt(data=train_spectrogram_ds.map(map_func=lambda spec, label: spec))\n\nmodel = models.Sequential([\n     layers.Input(shape=input_shape),\n    # Downsample the input.\n    layers.Resizing(32, 32),\n    # Normalize.\n    norm_layer,\n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels),\n])\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=['accuracy'],\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 0\nhistory = model.fit(\n    train_spectrogram_ds,\n    validation_data=val_spectrogram_ds,\n    epochs=EPOCHS,\n    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test the Models**","metadata":{}},{"cell_type":"code","source":"# If you want to loade a pre traind modell\n#classicModel = tf.keras.models.load_model(\"/kaggle/input/mylittlemodells/teLiteMod.h5\")\n#print(\"Done\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test normal Modell (selectedAudio is for all test)\nselectedAudio = \"/kaggle/input/mini-speech2/mini_speech_commands/right/0132a06d_nohash_1.wav\" # Audio File\n\nx = tf.io.read_file(selectedAudio)\nx, _ = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\nx = tf.squeeze(x, axis=-1)\nx = x[tf.newaxis, :]\n\ngenSpecto = get_spectrogram(x)\n\ntime_before = time()\nkeras_predic = classicModel.predict(genSpecto)\ntime_after = time()\ntotal_time = time_after - time_before\n\nclass_ids = tf.argmax(keras_predic, axis=-1)\nclass_names = tf.gather(label_names, class_ids)\n\ntest = 0.0\nfor i in keras_predic:\n    for j in i:\n        if j < 0:\n            x = 0\n            test += x\n        else:\n            test += j\n#print(test)\n\nfor i in keras_predic:\n    for j in i:\n        if j < 0:\n            x = 0\n            print(x*100/test)\n        else:\n            print(j*100/test)\n\n\nprint(\"The Prediction is:\", keras_predic)\nprint(\"The Prediction is:\", class_names.numpy()[0])\nprint(\"Total time:\", total_time)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Lite\n\n#Load file\n#tfLiteSimple = tf.lite.Interpreter(model_path=\"/kaggle/input/mylittlemodells/teLiteMod.tflite\")\ntfLiteSimple = tf.lite.Interpreter(model_path=\"/kaggle/input/mylittlemodells/teLiteMod_optim.tflite\")\n\ntfLiteSimple.allocate_tensors()\n\n#Input optput tensor\ninput_ten = tfLiteSimple.get_input_details()\noutput_ten = tfLiteSimple.get_output_details()\n#print(input_ten)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfLiteSimple.set_tensor(input_ten[0]['index'], genSpecto)\ntime_before = time()\nlite_predic = tfLiteSimple.invoke()\ntime_after = time()\ntotal_time = time_after - time_before\nout_data = tfLiteSimple.get_tensor(output_ten[0]['index']) \ntest = 0.0\nfor i in out_data:\n    for j in i:\n        if j < 0:\n            x = 0\n            test += x\n        else:\n            test += j\n#print(test)\n\nfor i in out_data:\n    for j in i:\n        if j < 0:\n            x = 0\n            print(x*100/test)\n        else:\n            print(j*100/test)\n\n\nprint(out_data)\nprint(\"Total time:\", total_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = history.history\nplt.figure(figsize=(16,6))\nplt.subplot(1,2,1)\nplt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.ylim([0, max(plt.ylim())])\nplt.xlabel('Epoch')\nplt.ylabel('Loss [CrossEntropy]')\n\nplt.subplot(1,2,2)\nplt.plot(history.epoch, 100*np.array(metrics['accuracy']), 100*np.array(metrics['val_accuracy']))\nplt.legend(['accuracy', 'val_accuracy'])\nplt.ylim([0, 100])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy [%]')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Evaluate Model**","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_spectrogram_ds, return_dict=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(test_spectrogram_ds)\ny_pred = tf.argmax(y_pred, axis=1)\ny_true = tf.concat(list(test_spectrogram_ds.map(lambda s,lab: lab)), axis=0)\n\nconfusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(confusion_mtx,\n            xticklabels=label_names,\n            yticklabels=label_names,\n            annot=True, fmt='g')\nplt.xlabel('Prediction')\nplt.ylabel('Label')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = data_dir/'no/01bb6a2a_nohash_0.wav'\nx = tf.io.read_file(str(x))\nx, sample_rate = tf.audio.decode_wav(x, desired_channels=1, desired_samples=16000,)\nx = tf.squeeze(x, axis=-1)\nwaveform = x\nx = create_mel_specto(x)\nx = x[tf.newaxis,...]\n\nprediction = model(x)\nx_labels = ['no', 'yes', 'down', 'go', 'left', 'up', 'right', 'stop']\nplt.bar(x_labels, tf.nn.softmax(prediction[0]))\nplt.title('No')\nplt.show()\n\ndisplay.display(display.Audio(waveform, rate=16000))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}